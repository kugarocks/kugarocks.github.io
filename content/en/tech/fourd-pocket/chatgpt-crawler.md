---
title: "ç”¨ ChatGPT å†™çˆ¬è™«"
description: ""
summary: ""
date: 2024-07-24T00:00:00+08:00
lastmod: 2024-09-08T00:00:00+08:00
weight: 500
seo:
  title: "ç”¨ ChatGPT å†™çˆ¬è™«"
  description: ""
  canonical: ""
  noindex: false
---

## ChatGPT Conversation

* ğŸ¤¡ï¼šç”¨ Python å†™çˆ¬è™«ï¼Œå¯è¾“å…¥ URL
* ğŸ¤–ï¼šç»™äº†ä¸€ä¸ª Beautiful Soup çš„ä¾‹å­
* ğŸ¤¡ï¼šæœ‰ä»€ä¹ˆæ–¹æ³•å¯æŠ“å– JS åŠ¨æ€ç”Ÿæˆçš„å†…å®¹
* ğŸ¤–ï¼šå¯ä½¿ç”¨ Seleniumã€Scrapy + Splash
* ğŸ¤¡ï¼šä½¿ç”¨ Scrapy + Splash çš„æ–¹æ³•
* ğŸ¤–ï¼šä»¥ä¸‹æ˜¯ä½¿ç”¨ Scrapy çš„è„šæœ¬ç¤ºä¾‹

è·‘äº†ä¸€ä¸‹ï¼Œå‘ç°æ—¥å¿—å…¨æ‰“åœ¨å±å¹•ä¸Šï¼Œäºæ˜¯å»å®˜ç½‘çœ‹äº†ä¸€ä¸‹é…ç½®ï¼Œå‘ç°å¯ä»¥æŒ‡å®šæ—¥å¿—æ–‡ä»¶ã€æŠ“å–æ·±åº¦ã€ä¼˜å…ˆçº§ç­‰ç­‰ã€‚
å†é—®ä¸€ä¸‹ GPT è®©ä»–ç»™ä¸€äº›é…ç½®çš„ä¾‹å­å’Œæ³¨é‡Šï¼Œæœ€åæ•´ç†ä¼˜åŒ–ä¸€ä¸‹ï¼Œå¾—å‡ºäº†ä¸‹é¢æœ€ç»ˆçš„è¾“å…¥ã€‚

```txt {frame="none"}
å†™ä¸€ä¸ª python çˆ¬è™«è„šæœ¬
- ä¸€å…± 4 ä¸ªè¾“å…¥å‚æ•°
- ç¬¬ 1 ä¸ªå‚æ•°ï¼šå¼€å§‹æŠ“å–çš„ URL
- ç¬¬ 2 ä¸ªå‚æ•°ï¼šåŒ¹é…çš„å…³é”®å­—ï¼Œæ”¯æŒå¤šä¸ª
- ç¬¬ 3 ä¸ªå‚æ•°ï¼šè®¾ç½®å¤šä¸ªå…³é”®å­—åŒ¹é…çš„ä¸æˆ–å…³ç³»
- ç¬¬ 4 ä¸ªå‚æ•°ï¼šè®¾ç½®çˆ¬è™«æŠ“å–çš„æ·±åº¦
- å½“ä¸€ä¸ªç½‘é¡µæˆåŠŸåŒ¹é…å¤šä¸ªå…³é”®å­—æ—¶ï¼Œåªè®°å½•ä¸€æ¬¡
- æŠŠåŒ¹é…å†…å®¹çš„ URL è¾“å‡ºåˆ°å±å¹•ï¼Œä¸€è¡Œä¸€ä¸ª
- ä½¿ç”¨ splash å’Œ scrapy æŠ€æœ¯
- scrapy é¡¹ç›®åç§°ä¸º mycrawler
- scrapy çš„æ—¥å¿—çº§åˆ«ä¸º INFOï¼Œæ—¥å¿—æ–‡ä»¶ä¸º spider.log
- scrapy çš„é…ç½®å•ç‹¬æ”¾åˆ° settings.py
- settings.py ä¸­æ·»åŠ æŠ“å–æ·±åº¦çš„ç›¸å…³é…ç½®
- settings.py ä¸­çš„æ¯ä¸ªé…ç½®æ·»åŠ ä¸­æ–‡æ³¨é‡Š
```

## ChatGPT Answer

ä¸‹é¢æ˜¯ ChatGPT çš„å›ç­”ï¼Œä½†æˆ‘åšäº†ä¸€äº›ä¿®æ”¹å’Œä¼˜åŒ–ã€‚

### å®‰è£… Splash

å®‰è£… Scrapy-Splash ä»¥åŠ Splash æœåŠ¡ï¼š

```bash {frame="none"}
pip install scrapy scrapy-splash
```

### åˆ›å»º Scrapy é¡¹ç›®

é¦–å…ˆï¼Œåœ¨å‘½ä»¤è¡Œä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„ Scrapy é¡¹ç›®ï¼š

```bash {frame="none"}
scrapy startproject mycrawler
```

### åœ¨ Docker è¿è¡Œ Splash

```bash {frame="none"}
docker run -p 8050:8050 scrapinghub/splash
```

### ä¿®æ”¹ settings.py

```python {frame="none"}
# -*- coding: utf-8 -*-

# Scrapy settings for mycrawler project

BOT_NAME = 'mycrawler'

SPIDER_MODULES = ['mycrawler.spiders']
NEWSPIDER_MODULE = 'mycrawler.spiders'

# ä½¿ç”¨ Splash æ¸²æŸ“ JavaScript é¡µé¢
SPLASH_URL = 'http://localhost:8050'

DOWNLOADER_MIDDLEWARES = {
    'scrapy_splash.SplashCookiesMiddleware': 723,
    'scrapy_splash.SplashMiddleware': 725,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}

SPIDER_MIDDLEWARES = {
    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,
}

DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'

# æŠ“å–æ·±åº¦è®¾ç½®
DEPTH_LIMIT = 3  # è®¾ç½®æŠ“å–æ·±åº¦é™åˆ¶
# æ¯æ¬¡æŠ“å–çš„æ·±åº¦
DEPTH_STATS = True  # å¯ç”¨æ·±åº¦ç»Ÿè®¡
# å¯ç”¨æŠ“å–æ·±åº¦ç»Ÿè®¡
DEPTH_PRIORITY = 1  # å¯ç”¨æŠ“å–æ·±åº¦ä¼˜å…ˆçº§
# è®¾ç½®æŠ“å–æ·±åº¦ä¼˜å…ˆçº§ï¼Œ1 è¡¨ç¤ºæ·±åº¦ä¼˜å…ˆï¼Œ-1 è¡¨ç¤ºå¹¿åº¦ä¼˜å…ˆ

# æ—¥å¿—è®¾ç½®
LOG_LEVEL = 'INFO'  # æ—¥å¿—çº§åˆ«
# è®¾ç½®æ—¥å¿—è®°å½•çš„çº§åˆ«ä¸º INFO
LOG_FILE = 'spider.log'  # æ—¥å¿—æ–‡ä»¶
# æŒ‡å®šæ—¥å¿—æ–‡ä»¶çš„è·¯å¾„

# å…¶ä»–é…ç½®
ROBOTSTXT_OBEY = True  # éµå®ˆ robots.txt è§„åˆ™
# çˆ¬è™«æ˜¯å¦éµå®ˆ robots.txt è§„åˆ™ï¼Œé»˜è®¤ä¸º True

# è®¾ç½® User-Agentï¼Œæ¨¡æ‹Ÿå¸¸è§æµè§ˆå™¨
#USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
```

### åˆ›å»ºçˆ¬è™«

åœ¨ `mycrawler/spiders` ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„çˆ¬è™«æ–‡ä»¶ `my_spider.py`ï¼Œå†…å®¹å¦‚ä¸‹ï¼š

```python {frame="none"}
import scrapy
from scrapy_splash import SplashRequest

class MySpider(scrapy.Spider):
    name = "my_spider"  # å®šä¹‰çˆ¬è™«çš„åç§°

    def __init__(self, start_url, keywords, logic, depth, *args, **kwargs):
        super(MySpider, self).__init__(*args, **kwargs)
        self.start_urls = [start_url]  # åˆå§‹åŒ–æŠ“å–çš„èµ·å§‹ URL
        self.keywords = keywords.split(',')  # å°†å…³é”®å­—å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºåˆ—è¡¨
        self.logic = logic  # è®¾ç½®å…³é”®å­—åŒ¹é…çš„é€»è¾‘å…³ç³»ï¼ˆä¸æˆ–å…³ç³»ï¼‰
        self.custom_settings = {'DEPTH_LIMIT': int(depth)}  # è®¾ç½®æŠ“å–æ·±åº¦é™åˆ¶

    def start_requests(self):
        # ç”Ÿæˆåˆå§‹è¯·æ±‚
        for url in self.start_urls:
            yield SplashRequest(url, self.parse, args={'wait': 1})

    def parse(self, response):
        # è§£æå“åº”å†…å®¹
        # è·å–é¡µé¢æ–‡æœ¬å¹¶è½¬ä¸ºå°å†™
        page_text = response.text.lower()
        # æ£€æŸ¥æ¯ä¸ªå…³é”®å­—æ˜¯å¦åœ¨é¡µé¢æ–‡æœ¬ä¸­å‡ºç°
        matches = [kw.lower() in page_text for kw in self.keywords]

        # æ ¹æ®é€»è¾‘å…³ç³»åˆ¤æ–­æ˜¯å¦åŒ¹é…æˆåŠŸ
        if (self.logic == 'and' and all(matches)) or (self.logic == 'or' and any(matches)):
            # è®°å½•åŒ¹é…çš„ URL
            self.log(f'Matched URL: {response.url}')  
            print(response.url)

        # è·å–é¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥ï¼Œå¹¶è¿›è¡Œé€’å½’æŠ“å–
        for next_page in response.css('a::attr(href)').getall():
            if next_page:
                yield SplashRequest(response.urljoin(next_page), self.parse, args={'wait': 1})
```

### è¿è¡Œçˆ¬è™«

ä½¿ç”¨å‘½ä»¤è¡Œè¿è¡Œçˆ¬è™«ï¼š

```bash {frame="none"}
scrapy crawl my_spider -a start_url='http://example.com' -a keywords='keyword1,keyword2' -a logic='and' -a depth=3
```

è¿™ä¸ªè„šæœ¬ä¼šæ ¹æ®è¾“å…¥å‚æ•°æŠ“å–ç½‘é¡µï¼ŒåŒ¹é…å…³é”®å­—å¹¶è¾“å‡ºåŒ¹é…çš„ URLã€‚

## æ€»ç»“

ChatGPT æ›´åƒæ˜¯ä¸€ä¸ªå¼•å¯¼çš„è§’è‰²ã€‚
æœ‰æ—¶å€™ä½ éœ€è¦ä¸€ä¸ªå·¥å…·ï¼Œå¯èƒ½ä½ å¹¶ä¸çŸ¥é“è¿™ä¸ªå·¥å…·æœ€åé•¿ä»€ä¹ˆæ ·å­ã€‚
ä½†åªè¦é€šè¿‡ä¸æ–­å¯¹è¯ï¼Œä¸€æ­¥æ­¥æ‰“ç£¨ï¼Œä»–æ€»èƒ½ç»™å‡ºä½ æƒ³è¦çš„ç­”æ¡ˆã€‚
è¿™ä¸ªè¿‡ç¨‹è¿˜èƒ½å­¦åˆ°ä¸å°‘çš„ä¸œè¥¿ï¼Œåªè¦ä½ æ‡‚å¾—å¦‚ä½•æé—®ã€‚
