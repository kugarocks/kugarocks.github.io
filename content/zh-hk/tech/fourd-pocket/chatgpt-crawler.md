---
title: "ç”¨ ChatGPT å¯«çˆ¬èŸ²"
description: ""
summary: ""
date: 2024-07-24T00:00:00+08:00
lastmod: 2024-09-08T00:00:00+08:00
weight: 500
seo:
  title: "ç”¨ ChatGPT å¯«çˆ¬èŸ²"
  description: ""
  canonical: ""
  noindex: false
---

## ChatGPT å°è©±

* ğŸ¤¡ï¼šç”¨ Python å¯«çˆ¬èŸ²ï¼Œå¯è¼¸å…¥ URL
* ğŸ¤–ï¼šç•€å’—ä¸€å€‹ Beautiful Soup å˜…ä¾‹å­
* ğŸ¤¡ï¼šæœ‰å’©æ–¹æ³•å¯æŠ“å– JS å‹•æ…‹ç”Ÿæˆå˜…å…§å®¹
* ğŸ¤–ï¼šå¯ä½¿ç”¨ Seleniumã€Scrapy + Splash
* ğŸ¤¡ï¼šä½¿ç”¨ Scrapy + Splash å˜…æ–¹æ³•
* ğŸ¤–ï¼šä»¥ä¸‹ä¿‚ä½¿ç”¨ Scrapy å˜…è…³æœ¬ç¤ºä¾‹

è·‘å’—ä¸€ä¸‹ï¼Œç™¼ç¾æ—¥èªŒå…¨æ‰“å–ºå±å¹•ä¸Šï¼Œæ–¼æ˜¯å»å®˜ç¶²ç‡å’—ä¸€ä¸‹é…ç½®ï¼Œç™¼ç¾å¯ä»¥æŒ‡å®šæ—¥èªŒæ–‡ä»¶ã€æŠ“å–æ·±åº¦ã€å„ªå…ˆç´šç­‰ç­‰ã€‚
å†å•ä¸€ä¸‹ GPT è®“ä½¢ç•€å•²é…ç½®å˜…ä¾‹å­åŒè¨»é‡‹ï¼Œæœ€å¾Œæ•´ç†å„ªåŒ–ä¸€ä¸‹ï¼Œå¾—å‡ºå’—ä¸‹é¢æœ€çµ‚å˜…è¼¸å…¥ã€‚

```txt {frame="none"}
å¯«ä¸€å€‹ python çˆ¬èŸ²è…³æœ¬
- ä¸€å…± 4 å€‹è¼¸å…¥åƒæ•¸
- ç¬¬ 1 å€‹åƒæ•¸ï¼šé–‹å§‹æŠ“å–å˜… URL
- ç¬¬ 2 å€‹åƒæ•¸ï¼šåŒ¹é…å˜…é—œéµå­—ï¼Œæ”¯æŒå¤šå€‹
- ç¬¬ 3 å€‹åƒæ•¸ï¼šè¨­ç½®å¤šå€‹é—œéµå­—åŒ¹é…å˜…èˆ‡æˆ–é—œä¿‚
- ç¬¬ 4 å€‹åƒæ•¸ï¼šè¨­ç½®çˆ¬èŸ²æŠ“å–å˜…æ·±åº¦
- ç•¶ä¸€å€‹ç¶²é æˆåŠŸåŒ¹é…å¤šå€‹é—œéµå­—æ™‚ï¼Œåªè¨˜éŒ„ä¸€æ¬¡
- æŠŠåŒ¹é…å…§å®¹å˜… URL è¼¸å‡ºåˆ°å±å¹•ï¼Œä¸€è¡Œä¸€å€‹
- ä½¿ç”¨ splash åŒ scrapy æŠ€è¡“
- scrapy é …ç›®åç¨±ç‚º mycrawler
- scrapy å˜…æ—¥èªŒç´šåˆ¥ç‚º INFOï¼Œæ—¥èªŒæ–‡ä»¶ç‚º spider.log
- scrapy å˜…é…ç½®å–®ç¨æ”¾åˆ° settings.py
- settings.py ä¸­æ·»åŠ æŠ“å–æ·±åº¦å˜…ç›¸é—œé…ç½®
- settings.py ä¸­å˜…æ¯å€‹é…ç½®æ·»åŠ ä¸­æ–‡è¨»é‡‹
```

## ChatGPT å›ç­”

ä¸‹é¢ä¿‚ ChatGPT å˜…å›ç­”ï¼Œä½†æˆ‘åšå’—å•²ä¿®æ”¹åŒå„ªåŒ–ã€‚

### å®‰è£ Splash

å®‰è£ Scrapy-Splash ä»¥åŠ Splash æœå‹™ï¼š

```bash {frame="none"}
pip install scrapy scrapy-splash
```

### å‰µå»º Scrapy é …ç›®

é¦–å…ˆï¼Œå–ºå‘½ä»¤è¡Œä¸­å‰µå»ºä¸€å€‹æ–°å˜… Scrapy é …ç›®ï¼š

```bash {frame="none"}
scrapy startproject mycrawler
```

### å–º Docker é‹è¡Œ Splash

```bash {frame="none"}
docker run -p 8050:8050 scrapinghub/splash
```

### ä¿®æ”¹ settings.py

```python {frame="none"}
# -*- coding: utf-8 -*-

# Scrapy settings for mycrawler project

BOT_NAME = 'mycrawler'

SPIDER_MODULES = ['mycrawler.spiders']
NEWSPIDER_MODULE = 'mycrawler.spiders'

# ä½¿ç”¨ Splash æ¸²æŸ“ JavaScript é é¢
SPLASH_URL = 'http://localhost:8050'

DOWNLOADER_MIDDLEWARES = {
    'scrapy_splash.SplashCookiesMiddleware': 723,
    'scrapy_splash.SplashMiddleware': 725,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}

SPIDER_MIDDLEWARES = {
    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,
}

DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'

# æŠ“å–æ·±åº¦è¨­ç½®
DEPTH_LIMIT = 3  # è¨­ç½®æŠ“å–æ·±åº¦é™åˆ¶
# æ¯æ¬¡æŠ“å–å˜…æ·±åº¦
DEPTH_STATS = True  # å•Ÿç”¨æ·±åº¦çµ±è¨ˆ
# å•Ÿç”¨æŠ“å–æ·±åº¦çµ±è¨ˆ
DEPTH_PRIORITY = 1  # å•Ÿç”¨æŠ“å–æ·±åº¦å„ªå…ˆç´š
# è¨­ç½®æŠ“å–æ·±åº¦å„ªå…ˆç´šï¼Œ1 è¡¨ç¤ºæ·±åº¦å„ªå…ˆï¼Œ-1 è¡¨ç¤ºå»£åº¦å„ªå…ˆ

# æ—¥èªŒè¨­ç½®
LOG_LEVEL = 'INFO'  # æ—¥èªŒç´šåˆ¥
# è¨­ç½®æ—¥èªŒè¨˜éŒ„å˜…ç´šåˆ¥ç‚º INFO
LOG_FILE = 'spider.log'  # æ—¥èªŒæ–‡ä»¶
# æŒ‡å®šæ—¥èªŒæ–‡ä»¶å˜…è·¯å¾‘

# å…¶ä»–é…ç½®
ROBOTSTXT_OBEY = True  # éµå®ˆ robots.txt è¦å‰‡
# çˆ¬èŸ²æ˜¯å¦éµå®ˆ robots.txt è¦å‰‡ï¼Œé»˜èªç‚º True

# è¨­ç½® User-Agentï¼Œæ¨¡æ“¬å¸¸è¦‹ç€è¦½å™¨
#USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
```

### å‰µå»ºçˆ¬èŸ²

å–º `mycrawler/spiders` ç›®éŒ„ä¸­å‰µå»ºä¸€å€‹æ–°å˜…çˆ¬èŸ²æ–‡ä»¶ `my_spider.py`ï¼Œå…§å®¹å¦‚ä¸‹ï¼š

```python {frame="none"}
import scrapy
from scrapy_splash import SplashRequest

class MySpider(scrapy.Spider):
    name = "my_spider"  # å®šç¾©çˆ¬èŸ²å˜…åç¨±

    def __init__(self, start_url, keywords, logic, depth, *args, **kwargs):
        super(MySpider, self).__init__(*args, **kwargs)
        self.start_urls = [start_url]  # åˆå§‹åŒ–æŠ“å–å˜…èµ·å§‹ URL
        self.keywords = keywords.split(',')  # å°‡é—œéµå­—å­—ç¬¦ä¸²æ‹†åˆ†ç‚ºåˆ—è¡¨
        self.logic = logic  # è¨­ç½®é—œéµå­—åŒ¹é…å˜…é‚è¼¯é—œä¿‚ï¼ˆèˆ‡æˆ–é—œä¿‚ï¼‰
        self.custom_settings = {'DEPTH_LIMIT': int(depth)}  # è¨­ç½®æŠ“å–æ·±åº¦é™åˆ¶

    def start_requests(self):
        # ç”Ÿæˆåˆå§‹è«‹æ±‚
        for url in self.start_urls:
            yield SplashRequest(url, self.parse, args={'wait': 1})

    def parse(self, response):
        # è§£æéŸ¿æ‡‰å…§å®¹
        # ç²å–é é¢æ–‡æœ¬ä¸¦è½‰ç‚ºå°å¯«
        page_text = response.text.lower()
        # æª¢æŸ¥æ¯å€‹é—œéµå­—æ˜¯å¦å–ºé é¢æ–‡æœ¬ä¸­å‡ºç¾
        matches = [kw.lower() in page_text for kw in self.keywords]

        # æ ¹æ“šé‚è¼¯é—œä¿‚åˆ¤æ–·æ˜¯å¦åŒ¹é…æˆåŠŸ
        if (self.logic == 'and' and all(matches)) or (self.logic == 'or' and any(matches)):
            # è¨˜éŒ„åŒ¹é…å˜… URL
            self.log(f'Matched URL: {response.url}')  
            print(response.url)

        # ç²å–é é¢ä¸­å˜…æ‰€æœ‰éˆæ¥ï¼Œä¸¦é€²è¡Œéæ­¸æŠ“å–
        for next_page in response.css('a::attr(href)').getall():
            if next_page:
                yield SplashRequest(response.urljoin(next_page), self.parse, args={'wait': 1})
```

### é‹è¡Œçˆ¬èŸ²

ä½¿ç”¨å‘½ä»¤è¡Œé‹è¡Œçˆ¬èŸ²ï¼š

```bash {frame="none"}
scrapy crawl my_spider -a start_url='http://example.com' -a keywords='keyword1,keyword2' -a logic='and' -a depth=3
```

å‘¢å€‹è…³æœ¬æœƒæ ¹æ“šè¼¸å…¥åƒæ•¸æŠ“å–ç¶²é ï¼ŒåŒ¹é…é—œéµå­—ä¸¦è¼¸å‡ºåŒ¹é…å˜… URLã€‚

## ç¸½çµ

ChatGPT æ›´ä¼¼ä¿‚ä¸€å€‹å¼•å°å˜…è§’è‰²ã€‚
æœ‰æ™‚ä½ éœ€è¦ä¸€å€‹å·¥å…·ï¼Œå¯èƒ½ä½ ä¸¦å””çŸ¥é“å‘¢å€‹å·¥å…·æœ€å¾Œé•·å’©æ¨£ã€‚
ä½†åªè¦é€šéä¸æ–·å°è©±ï¼Œä¸€æ­¥æ­¥æ‰“ç£¨ï¼Œä½¢ç¸½èƒ½ç•€å‡ºä½ æƒ³è¦å˜…ç­”æ¡ˆã€‚
å‘¢å€‹éç¨‹ä»²å¯ä»¥å­¸åˆ°å””å°‘å˜¢ï¼Œåªè¦ä½ æ‡‚å¾—é»æ¨£æå•ã€‚
